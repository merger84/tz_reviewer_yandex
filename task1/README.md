**Кейс 1**

Спринт — "Сложный SQL", студенту предстоит поисследовать данные, в данном случае хочется с помощью SQL решить поставленную задачу подсчёта среднего числа показов рекламы перед конверсией для пользователя

Уже изучено — SQL (joins, window-function, select, CTE)

![image](https://user-images.githubusercontent.com/16752576/169751823-21dc4664-f93d-43f7-8548-56317d04bc2a.png)

---

**Ответ**

Привет! Для начала я бы исключил все строки, где у пользователя время события типа show идёт после click. Для этого в условии where нужно ограничить время максимальной датой для события click этого же пользователя.

Далее используя оконную функцию count(событие) или rank()  over (partition by событие, пользователь order by время) пронумеруем строки сгруппировав по событиям и пользователю и отсортировав по времени.

Затем надо найти общие количества событий с типом show и click. Для этого нужно написать свой select, который будет выбирать max –имум из полученной оконной функции для каждого типа события с группировкой по пользователям.

И в конце нужно будет сджойнить получившиеся select-ы по пользователям. А полученные максимальные значения разделить один на второй (для show разделить на click).

---

**Кейс 2**

Спринт — "SQL", проходим window-functions

Уже изучено — SQL window function, limit, where

![image](https://user-images.githubusercontent.com/16752576/169751880-78b19d84-7d14-4c13-b137-1ab19dd779bf.png)

---

**Ответ**

Привет! Для подсчета топ значений внутри группы обычно используют одну из следующих оконных функций: row_number(), rank(), dense_rank() или при определенных условиях count(поле).

Например: select row_number() over (partition by ‘поля для группировки’ order by ‘поле по которому нужно сделать топ’) from table

Далее надо установить условие, ограничив результат оконной функции <= 5

---

**Кейс 3**

Спринт — "Витрины данных", студенты учились собирать витрины из данных в заданной модели DWH

Уже изучено — SQL, DWH, git

![image](https://user-images.githubusercontent.com/16752576/169751899-6bdeaa86-2909-45eb-9af1-0c4eb72f2792.png)

---

**Ответ**

Привет! Для того чтобы хранить код используют различные сервисы управления кодом. Чаще всего используют сервисы на основе GIT: GitHub или Gitlab. Так же можно встретить сервисы на основе SVN.

Основная задача GIT – это  отслеживание и ведения истории изменения файлов в проекте. Все данные хранятся на сервере, таким образом можно работать распределенной команде специалистов.

---

**Кейс 4**

Спринт — актуально в любом, пусть "Выгрузка данных из внешних источников на Python"

Уже изучено — Python, работа с файлами, pandas, работа с бд

![image](https://user-images.githubusercontent.com/16752576/169751937-80b7c5d1-eb6c-4b3f-ad03-35ba39dc842b.png)

---

**Ответ**

Привет! Чтобы найти ошибку, иногда приходится перебрать весь код пошагово. Начиная от источника данных, проверить те ли данные пришли к нам. Проверить каждую функцию на вывод результата. Хорошо при этом вести комментарии. 

Для ускорения поиска бага можно изменить входные данные, оставить например несколько строк входных данных из базы.

---

**Кейс 5**

Спринт — общий вопрос от студента

Уже изучено — Python

![image](https://user-images.githubusercontent.com/16752576/169751957-e81ebc10-9e52-460e-a8ff-01d41f0d4b47.png)

---

**Ответ**

Привет! Думаю в любой организации где ведется разработка нужен ревьюер. В нашей команде этим занимается архитектор. Он отслеживает качество кода, выявляет конфликтные ситуации в коде при совместной разработке, а также может проводить тесты с данными.

---

**Кейс 6**

Спринт — "Потоковая обработка данных", предстоит построить сложный пайплайн затрагивающий несколько источников и этапов обработки

Уже изучено — ETL, ELT, DWH, Data Lake, Spark, Airflow

![image](https://user-images.githubusercontent.com/16752576/169751974-9ed37140-b960-4a4d-a66d-d77a25e3a583.png)

---

**Ответ**

Привет! NiFi — это ETL/ELT-инструмент, который умеет работать со множеством систем, причем не только класса Big Data и Data Warehouse. 

NiFi позволяет парсить данные, выполнять по ним SQL, фильтровать и добавлять поля, конвертировать один формат данных в другой.

Для простоты восприятия в NiFi используется веб-интерфейс, в котором наглядно можно создавать всю цепочку  от получения данных, ее обработки до вывода конечному потребителю.

---

**Кейс 7**

Спринт — Streaming, вопрос про обработку ошибок в Kafka и ретраи

Уже изучено — Airflow, ETL, ELT, Spark, message queues, message broker

![image](https://user-images.githubusercontent.com/16752576/169751998-c244c04d-aaec-4b79-93bf-34fd3f0cc751.png)

---

**Ответ**

Привет!  Думаю нужно задать параметр с временем, когда нужно заново запустить MessageListenerContainer. Затем ищем его, выгружаем в JSON и читаем все «поломанные» сообщения.

---

**Кейс 8**

Спринт — "ETL процессы, Airflow"

Уже изучено — Python, DAG, SQL

![image](https://user-images.githubusercontent.com/16752576/169752005-895303a4-2198-4055-8cc2-3ae279b58707.png)

---

**Ответ**

Привет! Лучше всего все параметры вывести в отдельный json файл. И затем уже непосредственно вытягивать параметры.

---

**Кейс 9**

Спринт — "Python, data processing"

Уже изучено — Python, pandas, numpy, SQL

![image](https://user-images.githubusercontent.com/16752576/169752032-5b5823aa-df13-4f0b-af24-c82c6969bfac.png)

---

**Ответ**

Привет! Действительно, существует огромная проблема с выборкой данных, когда те или иные данные отсутствуют. Самый простой способ избавиться от таких ячеек – это заполнить их чем то, например значением по умолчанию. 

Например при помощи библиотеки pandas можно использовать следующее действие с датафрейом:

df = df.fillna(‘’) – это заполнит все ячейки с NaN на ''.

Можно использовать fillna() для отельной колонки:
df[column] = df[column].fillna(0)

После этого мы уже смело сможем обработать данные.

---
